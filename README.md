**Haptic Control ROS Package**

This repository contains a ROS node for using the PHANToM Omni to control the UR5 robotic arm in the simulated MoveIt! environment. It further allows the user to collect demonstrations of robotic manipulation around clutter, with point cloud scans of a tree displayed in simulation to model obstacles. Lastly, the code enables the validation of different path planning methods for solving the manipulation problem.

**Additional Requirements**

This node does not include the additional ROS packages required to run the PHANToM Omni or UR5 in simulation. However, these can be found at the following links and installed to the same Catkin workspace:
- Omni Node: https://github.com/danepowell/phantom_omni
- Omni Driver Installation Tutorial: https://github.com/jaejunlee0538/sensable_phantom_in_linux
- UR5 Node: https://github.com/ros-industrial/universal_robot

This node also includes further packages (ur5_control) that implement path planning and obstacle avoidance using OMPL. The folder ikfastpy may also need to be downloaded from the original source: https://github.com/andyzeng/ikfastpy

**Tele-operation and Demonstration Collection**

To control the UR5 using the PHANToM Omni and generate demonstrations of manipulation paths, execute these commands:
- To enable the Omni to communicate via FireWire, type in a new terminal the command: sudo chmod a+rw /dev/fw1
- Launch the Omni node via the command: roslaunch phantom_omni omni.launch
- In a new terminal launch the ur5 simulation: roslaunch ur5_moveit_config demo_apple.launch limited:=false
- In a new terminal launch the ur5 control nodes: roslaunch ur5_control scene.launch
- Set the desired orientation of the lidar scan by changing the angle in fixed_tf_broadcaster.py
- In a new terminal, add the lidar scan to the environment using the command: roslaunch haptic_control haptic_control.launch
- Ensure the self.run_validation variable in omni_to_ur5_sim.py is set to false
- In a new terminal, launch the tele-operation system using the command: rosrun haptic_control omni_to_ur5_sim.py

At this point, the UR5 will be sent to its home position and tele-operation can commence:
- Press the grey button on the Omni to enter tele-operation model: moving the Omni should now result in the UR5 end-effector moving similarly
- Press the grey button again to leave tele-operation. At this point, the UR5 will stay in its current position but moving the Omni will not change the UR5. To re-enter tele-operation mode, press the grey button again. To return to the home position and save the demonstrated trajectory, press the white button.
- The saved trajectory, goal and point cloud data will then be saved in the folder 'hatpic_control/training_trajectories/'. Ensure this folder exists or you will recieve an error. You may have to manually change the path defined in omni_to_ur5_sim.py to suit your system.

**Validation of Generated Trajectories**

This node also contains code for validating the trajectories generated by a Learning from Demonstration model trained on data generated using the tele-operation system. To run the validation code, follow these steps:
- Ensure the self.run_validation variable in omni_to_ur5_sim.py is set to true
- Place the LfD trajectories (simply a csv file containing a list of end-effector point) and the point cloud data and goal saved at the time of demonstration in the folder 'haptic_control/model validation traj/'. Again you may have to change the path defined in omni_to_ur5_sim.py to suit your system.
- Run the same commands required to enable tele-operation
- When the final command is run, the UR5 will again move to its home position. However, at this point it will begin executing the trajectories saved at the specified file location. 
- As the validation test is run, the execution time, planning time and details of LfD collisions will be saved in the folders 'haptic_control/model validation traj/rrt output/' and 'haptic_control/model validation traj/lfd output/'.
